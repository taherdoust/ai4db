================================================================================
DOWNLOAD STAGE 3 CHECKPOINT FROM IPAZIA126
================================================================================

Current Status (as of last check):
  - Processed: 6,200/49,783 samples (12.5%)
  - Generated: 22,294 augmented samples
  - Time per sample: 9.10 seconds
  - Estimated remaining: 110 hours (5-6 days)
  - Total estimated time: 125 hours

Performance Issue Identified:
  - Sequential OpenRouter API calls (not batched)
  - Each call takes ~9 seconds
  - This is MUCH slower than initially estimated (125 hours vs 3-5 hours)


OPTION 1: Download Latest Checkpoint (Recommended)
================================================================================
Download the checkpoint without stopping the running process on ipazia126.

# From your LOCAL machine:
cd /home/ali/Desktop/HDD_Volume/000products/coesi/ai4db/training_datasets

# Download checkpoint data (partial results, ~45MB)
scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage3_augmented_dataset_FINAL_checkpoint.jsonl ./

# Download checkpoint metadata (progress info, ~1KB)
scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage3_augmented_dataset_FINAL_checkpoint_meta.json ./

# Download log file (to see progress details, ~10MB)
scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/stage3.log ./stage3_partial.log


OPTION 2: Check Checkpoint Progress (Without Downloading)
================================================================================
# SSH to ipazia126
ssh castangia@ipazia126.polito.it

# Check how many samples in checkpoint
cd /media/space/castangia/Ali_workspace/ai4db/training_datasets
wc -l stage3_augmented_dataset_FINAL_checkpoint.jsonl

# Check checkpoint metadata
cat stage3_augmented_dataset_FINAL_checkpoint_meta.json | python3 -m json.tool

# Check latest log entries
tail -100 /media/space/castangia/Ali_workspace/ai4db/stage3.log


OPTION 3: Use Smaller Subset (RECOMMENDED FOR FASTER ITERATION)
================================================================================
If 125 hours is too long, use a subset of Stage 2 data:

# On ipazia126, stop current process
ps aux | grep stage3_augmentation
kill <PID>

# Create 10K subset (will take ~25 hours instead of 125)
cd /media/space/castangia/Ali_workspace/ai4db/training_datasets
head -10000 stage2_filtered.jsonl > stage2_filtered_10K.jsonl

# Run Stage 3 on subset
cd ..
nohup python stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
  --stage2-file training_datasets/stage2_filtered_10K.jsonl \
  --model "openai/gpt-4o-mini" \
  --multiplier 10 \
  --output-file training_datasets/stage3_augmented_dataset_10K.jsonl \
  > stage3_10K.log 2>&1 &

# Monitor
tail -f stage3_10K.log

Results with 10K subset:
  - Output: ~100,000 augmented samples (10K × 10 multiplier)
  - Time: ~25 hours
  - Cost: ~$4 USD
  - After curation: ~3K-4K training samples (still plenty for fine-tuning!)


OPTION 4: Resume on Local Machine
================================================================================
If you want to move processing to your local machine:

# Step 1: On ipazia126, stop the process
ps aux | grep stage3_augmentation
kill <PID>

# Step 2: Download checkpoint to local
cd /home/ali/Desktop/HDD_Volume/000products/coesi/ai4db/training_datasets
scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage3_augmented_dataset_FINAL_checkpoint.jsonl ./
scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage3_augmented_dataset_FINAL_checkpoint_meta.json ./
scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage2_filtered.jsonl ./

# Step 3: Resume on local machine
cd /home/ali/Desktop/HDD_Volume/000products/coesi/ai4db
conda activate aienv

nohup python stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
  --stage2-file training_datasets/stage2_filtered.jsonl \
  --model "openai/gpt-4o-mini" \
  --multiplier 10 \
  --output-file training_datasets/stage3_augmented_dataset_FINAL.jsonl \
  > stage3_resumed_local.log 2>&1 &


Estimated Completion Times:
================================================================================
Full dataset (49,783 samples):    ~125 hours (~5-6 days)
10K subset:                       ~25 hours (~1 day)
5K subset:                        ~12.5 hours (~0.5 day)

Current progress: 6,200/49,783 (12.5%)
Estimated completion: 5-6 days from now


Recommendations:
================================================================================
1. BEST OPTION: Stop current run, use 10K subset (25 hours)
   - Still produces 100K augmented samples
   - After curation: ~3K-4K training samples (sufficient for fine-tuning)
   - Much faster iteration

2. ALTERNATIVE: Let ipazia126 continue with full dataset
   - Will take 5-6 days total
   - Produces 497K augmented samples
   - After curation: ~15K-20K training samples

3. Backup checkpoints periodically (every 12-24 hours)
   - Protects against crashes/interruptions
   - Allows you to analyze partial results


Verify Downloaded Checkpoint:
================================================================================
# Check sample count
wc -l stage3_augmented_dataset_FINAL_checkpoint.jsonl

# Check first sample
head -1 stage3_augmented_dataset_FINAL_checkpoint.jsonl | python3 -m json.tool

# Check metadata
cat stage3_augmented_dataset_FINAL_checkpoint_meta.json | python3 -m json.tool


Why is Stage 3 So Slow?
================================================================================
The bottleneck is sequential OpenRouter API calls:
  - Each API call: ~9 seconds (includes network latency, API processing)
  - No batching: Processes one sample at a time
  - 49,783 samples × 9 sec = 447,047 seconds = 124 hours

Possible future improvements (not implemented yet):
  - Batch API calls (5-10 concurrent requests)
  - Use faster models (but lower quality)
  - Reduce multiplier (8x instead of 10x)

For now: Use subset approach for faster iteration!
================================================================================


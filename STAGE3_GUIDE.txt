╔═══════════════════════════════════════════════════════════════════════════╗
║                                                                           ║
║   🎉 STAGE 2 COMPLETE - 99.57% NoErr! (49,783/50,000 passing)           ║
║   🚀 READY FOR STAGE 3 - GPT-4o-mini Augmentation                       ║
║                                                                           ║
╚═══════════════════════════════════════════════════════════════════════════╝

┌───────────────────────────────────────────────────────────────────────────┐
│ STAGE 2 RESULTS SUMMARY                                                   │
└───────────────────────────────────────────────────────────────────────────┘

Progress Through Fixes:
  Iteration 1:   0.00% NoErr  →  Fixed table names (dtm/dsm)
  Iteration 2:   4.35% NoErr  →  Fixed aliases/WHERE clauses  
  Iteration 3:  22.52% NoErr  →  Fixed some ID columns
  Iteration 4:  99.57% NoErr  →  COMPREHENSIVE FIX ✅

Final Breakdown by SQL Type:
  ✅ AGGREGATION:         100.0% (6,674/6,674)
  ✅ SIMPLE_SELECT:       100.0% (13,583/13,583)
  ✅ SPATIAL_JOIN:        100.0% (1,630/1,630)
  ✅ SPATIAL_MEASUREMENT: 100.0% (9,373/9,374)
  ✅ SPATIAL_CLUSTERING:   99.0% (203/205)
  ✅ MULTI_JOIN:           99.0% (7,434/7,511)
  ✅ RASTER_VECTOR:        98.9% (5,762/5,825)
  ✅ NESTED_QUERY:         98.6% (5,124/5,198)

Key Success Factors:
  1. Correct ID column mapping per table (building_id, bus_id, etc.)
  2. Strict complexity control (EASY=1 table)
  3. Geometry awareness (skipped invalid spatial joins)
  4. PyTorch CUDA 12.1 (GPU: Quadro RTX 6000)

Files Ready:
  ✅ stage2_synthetic_dataset_ipazia.jsonl (50,000 total)
  ✅ stage2_filtered.jsonl (49,783 passing - USE THIS!) ⭐
  ✅ stage2_quality_report.json

┌───────────────────────────────────────────────────────────────────────────┐
│ STAGE 3 WORKFLOW - 3 TERMINALS                                            │
└───────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════
TERMINAL 1: Transfer Files (Local Machine)
═══════════════════════════════════════════════════════════════════════════

cd /home/ali/Desktop/HDD_Volume/000products/coesi/ai4db

# 1. Transfer .env (has GPT-4o-mini config)
scp .env castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/

# 2. Transfer Stage 3 script
scp stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
    castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/

# 3. Transfer filtered dataset (73MB, 49,783 samples)
scp training_datasets/stage2_filtered.jsonl \
    castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/

echo "✅ Transfer complete!"


═══════════════════════════════════════════════════════════════════════════
TERMINAL 2: Start Stage 3 (SSH to ipazia126)
═══════════════════════════════════════════════════════════════════════════

ssh castangia@ipazia126.polito.it
cd /media/space/castangia/Ali_workspace/ai4db
conda activate ai4cimdb

# Verify .env file
cat .env | grep OPENROUTER
# Should show:
#   OPENROUTER_API_KEY=sk-or-v1-...
#   OPENROUTER_MODEL=openai/gpt-4-mini

# Verify input file (49,783 lines, 73MB)
ls -lh training_datasets/stage2_filtered.jsonl
wc -l training_datasets/stage2_filtered.jsonl

# Run Stage 3 in background
nohup python stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
  --stage2-file training_datasets/stage2_filtered.jsonl \
  --model "openai/gpt-4o-mini" \
  --multiplier 10 \
  --output-file training_datasets/stage3_augmented_dataset_FINAL.jsonl \
  > stage3.log 2>&1 &

# Get process ID (SAVE THIS!)
STAGE3_PID=$!
echo "Stage 3 PID: $STAGE3_PID"

# Wait 10 seconds, then check initial output
sleep 10
tail -30 stage3.log

# You can disconnect now (process continues in background)
# To disconnect: Ctrl+D or type 'exit'


═══════════════════════════════════════════════════════════════════════════
TERMINAL 3: Monitor Progress (From ANY Machine)
═══════════════════════════════════════════════════════════════════════════

# Connect to ipazia126
ssh castangia@ipazia126.polito.it
cd /media/space/castangia/Ali_workspace/ai4db

# Real-time monitoring (press Ctrl+C to stop)
tail -f stage3.log

# Useful monitoring commands (run after Ctrl+C):
# ─────────────────────────────────────────────────────────────────────────

# Check how many samples processed
grep "Processed" stage3.log | tail -1

# Check checkpoint progress
grep "Checkpoint saved" stage3.log | tail -3

# Count lines in latest checkpoint file
ls -lht training_datasets/stage3_augmented_dataset_FINAL_checkpoint*.jsonl | head -1
wc -l training_datasets/stage3_augmented_dataset_FINAL_checkpoint*.jsonl 2>/dev/null | tail -1

# Check if process is still running
ps aux | grep stage3_augmentation | grep -v grep

# Check API call count
grep "API calls" stage3.log | tail -1

# Check estimated cost
grep "Cost" stage3.log | tail -1

# Check memory usage
free -h


═══════════════════════════════════════════════════════════════════════════
EXPECTED TIMELINE & COST
═══════════════════════════════════════════════════════════════════════════

Input:              49,783 samples (99.57% NoErr from Stage 2)
Multiplier:         10x
Output:             ~497,830 (question, instruction, SQL) triples
Model:              GPT-4o-mini
Cost:               $7.50-15 USD
Time:               3-5 hours
Checkpoints:        Every 1,000 samples (498 total)
Progress rate:      ~8K-12K samples/hour

Milestones:
  00:00  →  Stage 3 starts
  00:10  →  First checkpoint (1,000 samples)
  01:00  →  ~10,000 samples processed
  02:00  →  ~20,000 samples processed
  03:00  →  ~30,000 samples processed
  04:00  →  ~40,000 samples processed
  05:00  →  COMPLETE! (~497,830 samples)


═══════════════════════════════════════════════════════════════════════════
IF STAGE 3 STOPS/CRASHES
═══════════════════════════════════════════════════════════════════════════

# Check if process died
ps aux | grep stage3 | grep -v grep

# Check last output in log
tail -50 stage3.log

# Resume from last checkpoint (automatic!)
nohup python stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
  --stage2-file training_datasets/stage2_filtered.jsonl \
  --model "openai/gpt-4o-mini" \
  --multiplier 10 \
  --output-file training_datasets/stage3_augmented_dataset_FINAL.jsonl \
  > stage3_resumed.log 2>&1 &

# Script automatically detects and loads the latest checkpoint!


═══════════════════════════════════════════════════════════════════════════
WHEN STAGE 3 COMPLETES
═══════════════════════════════════════════════════════════════════════════

# Verify output file exists
ls -lh training_datasets/stage3_augmented_dataset_FINAL.jsonl

# Check sample count (should be ~497,830)
wc -l training_datasets/stage3_augmented_dataset_FINAL.jsonl

# View statistics
tail -100 stage3.log

# Transfer back to local machine (run from local terminal)
cd /home/ali/Desktop/HDD_Volume/000products/coesi/ai4db/training_datasets

scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage3_augmented_dataset_FINAL.jsonl ./

scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/training_datasets/stage3_augmented_dataset_FINAL_checkpoint_meta.json ./

scp castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/stage3.log ./


═══════════════════════════════════════════════════════════════════════════
NEXT STEPS AFTER STAGE 3
═══════════════════════════════════════════════════════════════════════════

1. Dataset Curation (15-30 minutes):
   cd /media/space/castangia/Ali_workspace/txt2ssql/fine-tune
   
   python curate_cim_dataset.py \
     ../../ai4db/training_datasets/stage3_augmented_dataset_FINAL.jsonl \
     --output_dir curated_dataset \
     --quality_threshold 0.75
   
   python clean_curated_dataset.py \
     --input_dir curated_dataset \
     --output_dir curated_dataset_clean

2. Fine-tuning Llama 3.1 14B (7-9 hours):
   nohup python train_llama_14b_cim_spatial_sql.py --lora_rank r16 \
     > training.log 2>&1 &

3. Evaluation (1-2 hours):
   python evaluate_models.py \
     --benchmark ../ai4db/evaluation_benchmark.jsonl \
     --model hf:taherdoust/llama-3.1-14b-cim-spatial-sql \
     --metric EX


╔═══════════════════════════════════════════════════════════════════════════╗
║                                                                           ║
║   📊 EXPECTED FINAL RESULTS                                              ║
║                                                                           ║
║   Stage 3 Output:        ~497,830 samples                                ║
║   After Curation:        ~15K-20K training samples                       ║
║   Fine-tuned Model EX:   88-95% (first-shot accuracy)                    ║
║   Fine-tuned Model EA:   92-98% (with agent mode)                        ║
║   Total Time:            12-17 hours (Stage 3 → Fine-tuned model)        ║
║   Total Cost:            $7.50-15 USD (Stage 3 only)                     ║
║                                                                           ║
╚═══════════════════════════════════════════════════════════════════════════╝


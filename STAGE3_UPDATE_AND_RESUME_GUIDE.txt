================================================================================
STAGE 3: UPDATE SCRIPT AND RESUME FROM CHECKPOINT
================================================================================

Date: October 29, 2025
Current Progress: ~30,000 samples (6,000 input SQLs × 3.6-5 variations)
Remaining: ~150,000 samples

================================================================================
WHAT WAS CHANGED IN stage3_augmentation_pipeline_eclab_openrouter_enhanced.py
================================================================================

1. Question Max Length: 300 → 500 characters
   - Line 620: max_length: int = 500
   - Allows longer, more detailed natural language questions
   - Matches curation pipeline limits (curate_cim_dataset.py)

2. Instruction Max Length: 800 → 1200 characters
   - Line 621: max_instruction_length: int = 1200
   - Allows comprehensive step-by-step decomposition
   - Provides richer training signal for instruction-following

3. Enhanced Tone Diversity Prompting
   - Lines 452-462: Added explicit 6-tone examples in LLM prompt:
     * INTERROGATIVE: "What are...", "Which buildings..."
     * DIRECT: "Find all...", "Show me..."
     * ANALYTICAL: "Analyze the...", "Determine which..."
     * AGGREGATE: "Count how many...", "Sum the total..."
     * SPATIAL_SPECIFIC: "Buildings within...", "Areas intersecting..."
     * DESCRIPTIVE: "Can you retrieve...", "I need to know..."
   - Result: More balanced tone distribution (not just INTERROGATIVE/DESCRIPTIVE)

4. Increased LLM Token Budget
   - Line 478: max_tokens: 1000 (was 600)
   - Accommodates longer question-instruction pairs
   - Prevents truncation of detailed instructions

================================================================================
STEP-BY-STEP GUIDE TO UPDATE AND RESUME
================================================================================

STEP 1: STOP CURRENT STAGE 3 ON IPAZIA126
------------------------------------------
ssh castangia@ipazia126.polito.it

# Find the process
ps aux | grep stage3_augmentation

# Output example:
# castangia  123456  ... python stage3_augmentation_pipeline_eclab_openrouter_enhanced.py ...

# Kill gracefully (PID is the second column)
kill 123456

# Wait 10-15 seconds, verify it stopped
ps aux | grep stage3_augmentation
# (should show nothing or just the grep command itself)


STEP 2: BACKUP CURRENT CHECKPOINT (SAFETY!)
--------------------------------------------
cd /media/space/castangia/Ali_workspace/ai4db/training_datasets

# Create backup of current progress (~30K samples)
cp stage3_augmented_dataset_FINAL_checkpoint.jsonl \
   stage3_augmented_dataset_FINAL_checkpoint_BACKUP_30K.jsonl

cp stage3_augmented_dataset_FINAL_checkpoint_meta.json \
   stage3_augmented_dataset_FINAL_checkpoint_meta_BACKUP_30K.json

# Verify backup
ls -lh *BACKUP*

# Expected output:
# -rw-r--r-- 1 castangia ... stage3_augmented_dataset_FINAL_checkpoint_BACKUP_30K.jsonl
# -rw-r--r-- 1 castangia ... stage3_augmented_dataset_FINAL_checkpoint_meta_BACKUP_30K.json


STEP 3: TRANSFER UPDATED SCRIPT FROM LOCAL TO IPAZIA126
--------------------------------------------------------
# ON LOCAL MACHINE (eclab)
cd /home/ali/Desktop/HDD_Volume/000products/coesi/ai4db

# Transfer updated script
scp stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
  castangia@ipazia126.polito.it:/media/space/castangia/Ali_workspace/ai4db/

# You'll see:
# stage3_augmentation_pipeline_eclab_openrouter_enhanced.py    100%  ...KB   ...MB/s


STEP 4: VERIFY UPDATE ON IPAZIA126
-----------------------------------
ssh castangia@ipazia126.polito.it
cd /media/space/castangia/Ali_workspace/ai4db

# Check that file was updated (timestamp should be recent)
ls -lh stage3_augmentation_pipeline_eclab_openrouter_enhanced.py

# Optionally, verify the changes
grep "max_length: int = 500" stage3_augmentation_pipeline_eclab_openrouter_enhanced.py
grep "max_instruction_length: int = 1200" stage3_augmentation_pipeline_eclab_openrouter_enhanced.py

# Both should show matches


STEP 5: RESUME STAGE 3 WITH UPDATED SCRIPT
-------------------------------------------
cd /media/space/castangia/Ali_workspace/ai4db
conda activate ai4cimdb

# Resume - script will automatically detect checkpoint!
nohup python stage3_augmentation_pipeline_eclab_openrouter_enhanced.py \
  --stage2-file training_datasets/stage2_filtered.jsonl \
  --model "openai/gpt-4o-mini" \
  --multiplier 10 \
  --output-file training_datasets/stage3_augmented_dataset_FINAL.jsonl \
  > stage3_FINAL_RESUMED.log 2>&1 &

# Get the process ID
echo $!
# Output: 234567 (your new PID)

# Save this PID for monitoring


STEP 6: MONITOR PROGRESS
-------------------------
# Check it's running
ps aux | grep stage3_augmentation

# Monitor log file
tail -f stage3_FINAL_RESUMED.log

# You should see:
# [OK] Found existing checkpoint at training_datasets/stage3_augmented_dataset_FINAL_checkpoint.jsonl
# [OK] Resuming from sample index 5999 (6000 samples already processed)
# [OK] Resuming from 30000 augmented samples
# ...

# Check metadata (updated every 1,000 samples)
cat training_datasets/stage3_augmented_dataset_FINAL_checkpoint_meta.json


STEP 7: VERIFICATION (AFTER ~15-30 MINUTES)
--------------------------------------------
# Check if new samples are being generated with updated limits
tail -100 training_datasets/stage3_augmented_dataset_FINAL_checkpoint.jsonl | \
  python3 -c "
import json
import sys
for line in sys.stdin:
    sample = json.loads(line)
    q_len = len(sample['question'])
    i_len = len(sample['instruction'])
    tone = sample.get('question_tone', 'UNKNOWN')
    print(f'Question: {q_len} chars | Instruction: {i_len} chars | Tone: {tone}')
" | tail -20

# You should see:
# - Questions up to 500 chars (some >300)
# - Instructions up to 1200 chars (some >800)
# - More diverse tones (not just INTERROGATIVE/DESCRIPTIVE)


================================================================================
WHAT WILL HAPPEN WHEN YOU RESUME
================================================================================

1. ✅ Script detects existing checkpoint file
2. ✅ Reads last_processed_idx from metadata (probably ~5999)
3. ✅ Reads existing 30K samples into memory
4. ✅ Resumes from next SQL (index 6000)
5. ✅ Applies NEW limits to all future samples:
   - Questions: 20-500 chars (instead of 20-300)
   - Instructions: 20-1200 chars (instead of 20-800)
   - Enhanced tone diversity prompting
6. ✅ Saves checkpoint every 1,000 samples as usual

Result:
- Existing samples (0-30K): Old limits (300 chars questions, 800 chars instructions)
- Future samples (30K-180K): NEW limits (500 chars questions, 1200 chars instructions)
- Net benefit: ~83% of samples (150K/180K) will use improved limits!


================================================================================
EXPECTED TIMELINE
================================================================================

Current progress:    ~30,000 samples (17% complete)
Remaining samples:   ~150,000 samples
Time per sample:     ~9 seconds
Remaining time:      ~375 hours (15-16 days)

Total project time:  ~500 hours (21 days) for full 180K samples

Alternative: Use 10K subset approach (~36K final samples, 25 hours total)


================================================================================
RECOVERY PLAN (IF SOMETHING GOES WRONG)
================================================================================

If the resume fails or produces errors:

# Restore backup
cd /media/space/castangia/Ali_workspace/ai4db/training_datasets

cp stage3_augmented_dataset_FINAL_checkpoint_BACKUP_30K.jsonl \
   stage3_augmented_dataset_FINAL_checkpoint.jsonl

cp stage3_augmented_dataset_FINAL_checkpoint_meta_BACKUP_30K.json \
   stage3_augmented_dataset_FINAL_checkpoint_meta.json

# Verify restoration
ls -lh stage3_augmented_dataset_FINAL_checkpoint*

# Then try again or contact for help


================================================================================
BENEFITS OF UPDATING NOW (WITH 30K SAMPLES ALREADY DONE)
================================================================================

✅ Low Risk: Checkpoint system is designed for this
✅ Future Benefits: 83% of samples get improved limits
✅ More Diverse: Better tone distribution going forward
✅ Aligned: Matches curation pipeline (500 char questions)
✅ Richer Instructions: 1200 char limit for detailed decomposition
✅ No Data Loss: All 30K existing samples preserved

Downside: 30K samples already generated with old limits (but still high quality!)


================================================================================
MONITORING COMMANDS (KEEP HANDY)
================================================================================

# Check if running
ps aux | grep stage3_augmentation

# Monitor live log
tail -f stage3_FINAL_RESUMED.log

# Check progress (sample count)
wc -l training_datasets/stage3_augmented_dataset_FINAL_checkpoint.jsonl

# Check metadata
cat training_datasets/stage3_augmented_dataset_FINAL_checkpoint_meta.json | jq .

# Check recent samples
tail -5 training_datasets/stage3_augmented_dataset_FINAL_checkpoint.jsonl | jq .

# Kill if needed (gracefully)
kill <PID>

# Kill if stuck (forcefully)
kill -9 <PID>


================================================================================
NEXT STEPS AFTER STAGE 3 COMPLETES
================================================================================

1. Download final checkpoint to local machine
2. Run curation (single-step integrated script):
   python curate_cim_dataset.py \
     stage3_augmented_dataset_FINAL_checkpoint.jsonl \
     --output_dir curated_dataset_clean \
     --quality_threshold 0.75 \
     --max_question_length 500 \
     --keep_fields id question instruction sql_postgis

3. Expected output: ~88K-113K training samples (70-90% retention)
4. Start fine-tuning on ipazia126 (7-9 hours)
5. Evaluate with EM, EX, EA metrics


================================================================================
SUMMARY
================================================================================

Changes Applied:
- Question max: 300 → 500 chars
- Instruction max: 800 → 1200 chars  
- Tone diversity: Generic → 6 explicit tones
- LLM tokens: 600 → 1000

Process:
1. Stop current Stage 3 on ipazia126
2. Backup checkpoint (30K samples)
3. Transfer updated script from local to ipazia126
4. Resume - script auto-detects checkpoint
5. Future samples use new improved limits

Expected Result:
- 30K samples with old limits (already high quality)
- 150K samples with NEW limits (83% benefit from improvements)
- Total: ~180K samples with better question/instruction quality

================================================================================


\section{Methodology}
\label{sec:method}

\subsection{Evaluation Dataset} litrature review on how to create dataset for text to spatial sql: 1-human generated: which needs to keep track of question, sql from an existing base platform or huge labor cost 2- augmented dataset: 2-a ontology pipeline to syntethize a rulebase generated pairs 2-b distilting techniques to create natural language questions and sql pair, there are two different approaches in this area 2-b-1 is to create natural language questions first by a rule based method then generate the sql statements by closed source LLM model which could be costly 2-b-2 create a rule based spatial sql then generate the natural language question, since natural language has more flexibility regard to sql statements.

The methodology of this study is grounded in the CIM Wizard database, a geospatial platform designed for energy and urban infrastructure analysis. The schema integrates multiple geometry types, including polygons and multipolygons for project and census boundaries, building footprints, points for grid buses and project centers, and linestrings for grid lines. Non-spatial attributes such as building height, area, and energy-related properties complement the spatial information. This diversity makes CIM Wizard an ideal testbed for exploring the challenges of Text-to-Spatial-SQL.

The dataset constructed for this study is schema-aware, as it is built directly from the CIM Wizard schema. Each entry consists of a natural language question paired with a ground-truth PostGIS SQL query. Queries are executed during curation to record results, and each example is annotated with the spatial functions used and the corresponding difficulty level. Functions include containment predicates such as \texttt{ST\_Within}, proximity operators such as \texttt{ST\_DWithin}, measurement functions such as \texttt{ST\_Area} and \texttt{ST\_Length}, and transformation functions such as \texttt{ST\_Transform}. Difficulties range from simple single-table queries to multi-join and nested spatial operations.

A fine-tuning strategy is applied to an open-source large language model, where the model learns to map directly from natural language questions to SQL queries tailored to the CIM Wizard schema. Fine-tuning is performed with parameter-efficient methods such as LoRA, allowing the model to specialize in spatial SQL without requiring extensive computational resources. The trained model is then integrated into CIM Wizard as a backend service. Users interact through a natural-language interface, and the system returns executable PostGIS SQL queries and their corresponding results, thereby enabling non-SQL experts to formulate complex spatial queries.

Evaluation of the system focuses on execution accuracy, defined as the equivalence between generated and ground-truth query results. In addition, spatial correctness can be validated through geometric equivalence tests, such as measuring the area of the symmetric difference between result geometries. Error analysis is also carried out, categorizing common mistakes such as selecting incorrect predicates, omitting coordinate system transformations, or referencing the wrong geometry columns.

\subsection{Dataset Creation Methodology}
\label{subsec:dataset-creation}

This section presents our comprehensive approach to generating high-quality spatial SQL training datasets for Large Language Model fine-tuning, employing empirically-validated function selection and rule-based template generation with schema-specific extensions.

\subsubsection{Spatial Function Selection Strategy}
\label{subsubsec:function-selection}

Our spatial function selection strategy is grounded in empirical evidence from the SpatialSQL benchmark~\cite{gao2024text}, which provides the first systematic analysis of spatial function usage patterns in real-world applications. Through analysis of 200 spatial queries across four databases (ada, edu, tourism, traffic), Gao et al.~\cite{gao2024text} demonstrated that spatial function usage follows a highly concentrated distribution, with only 14 functions (2\% of PostGIS's 650+ functions) handling real-world spatial query requirements.

\paragraph{Empirical Foundation}
The SpatialSQL benchmark reveals that the top 5 spatial functions account for 75.2\% of all spatial operations:
\begin{itemize}
    \item \texttt{ST\_Intersects()}: 18.9\% usage -- Most critical spatial predicate
    \item \texttt{ST\_Area()}: 17.3\% usage -- Essential measurement function
    \item \texttt{ST\_Distance()}: 14.2\% usage -- Core proximity analysis
    \item \texttt{ST\_Contains()}: 13.0\% usage -- Fundamental containment test
    \item \texttt{ST\_Within()}: 11.8\% usage -- Inverse containment relationship
\end{itemize}

\paragraph{Conservative Coverage Approach}
While the empirical evidence demonstrates that 14 functions suffice for most spatial applications, our pipeline includes \textbf{76 spatial functions} (10\% of PostGIS functions), providing \textbf{5.4$\times$ more comprehensive coverage} than empirically demonstrated needs. This conservative approach ensures robust LLM training across diverse spatial analysis scenarios while maintaining efficiency.

Our function selection employs a five-tier priority hierarchy based on empirical usage patterns:
\begin{itemize}
    \item \textbf{CRITICAL} (5 functions): Top-tier functions from SpatialSQL analysis
    \item \textbf{VERY\_HIGH} (7 functions): Second-tier empirical functions plus pedagogically essential operations
    \item \textbf{HIGH} (10 functions): Common workflow functions with moderate usage
    \item \textbf{MEDIUM} (8 functions): Specialized analysis functions
    \item \textbf{LOW} (46 functions): Advanced and domain-specific functions
\end{itemize}

\subsubsection{Sample Generation Capacity and Scalability}
\label{subsubsec:sample-generation}

Our pipeline employs a two-tier architecture that generates training samples through systematic template expansion with parameter variation. The system comprises 52 unique spatial SQL templates distributed across three complexity levels, enabling scalable dataset generation for different model sizes and training requirements.

\paragraph{Template Distribution}
The template inventory is structured as follows:
\begin{itemize}
    \item \textbf{Base Rule-Based Generator}: 24 templates covering fundamental spatial operations
    \item \textbf{CIM Wizard Schema-Specific Generator}: 28 templates for realistic database integration
    \item \textbf{Total Unique Templates}: 52 templates with dual-dialect support (PostGIS/SpatiaLite)
\end{itemize}

\paragraph{Generation Capacity}
Through systematic parameter variation, our pipeline supports scalable dataset generation:
\begin{itemize}
    \item \textbf{Small Dataset} (10 variations): ~520 samples -- Suitable for initial experimentation
    \item \textbf{Medium Dataset} (50 variations): ~2,600 samples -- Appropriate for model validation
    \item \textbf{Large Dataset} (200 variations): ~10,400 samples -- Recommended for 7B model training
    \item \textbf{Production Scale} (1,000 variations): ~52,000 samples -- Optimal for 14B/32B model fine-tuning
\end{itemize}

The scalable approach aligns with QLoRA fine-tuning requirements~\cite{dettmers2023qlora}, where larger models benefit from more diverse training examples while maintaining parameter efficiency.

\subsubsection{Spatial Function Distribution in Generated Dataset}
\label{subsubsec:function-distribution}

Our generated dataset reflects empirically-validated spatial function usage patterns while ensuring comprehensive coverage for robust LLM training. The function distribution strategy balances real-world relevance with pedagogical completeness.

\paragraph{Usage-Weighted Template Generation}
Templates are weighted according to empirical usage patterns from the SpatialSQL benchmark, ensuring that critical spatial operations receive proportionally more training examples:
\begin{itemize}
    \item \textbf{Relationship predicates} (48.6\% of operations): Emphasis on \texttt{ST\_Intersects}, \texttt{ST\_Contains}, \texttt{ST\_Within}
    \item \textbf{Measurement functions} (40.2\% of operations): Focus on \texttt{ST\_Area}, \texttt{ST\_Distance}, \texttt{ST\_Length}
    \item \textbf{Overlay operations} (6.8\% of operations): Coverage of \texttt{ST\_Intersection}, \texttt{ST\_Union}
    \item \textbf{Processing functions} (4.4\% of operations): Include \texttt{ST\_Centroid}, \texttt{ST\_Buffer}, spatial indexing
\end{itemize}

\paragraph{Enhanced Metadata Structure}
Each generated sample includes comprehensive metadata enabling sophisticated training analysis:
\begin{lstlisting}[language=json,caption={Enhanced training sample structure}]
{
  "id": "template_identifier",
  "instruction": "Convert this natural language description to spatial SQL: ...",
  "input": "Natural language spatial query description",
  "output_postgis": "PostGIS-compatible SQL query",
  "output_spatialite": "SpatiaLite-compatible SQL query",
  "complexity": "A|B|C",
  "usage_index": "frequency_level:function_type",
  "evidence": {
    "database": "schema_identifier",
    "schemas": ["schema_names"],
    "tables": ["qualified_table_names"],
    "columns": ["column_names"],
    "functions": ["spatial_functions_used"],
    "template_source": "generation_source"
  }
}
\end{lstlisting}

\subsubsection{Rule-Based SQL Generation Architecture}
\label{subsubsec:rule-based-generation}

Our rule-based generation methodology employs systematic template expansion with intelligent parameter substitution, ensuring syntactic correctness and semantic validity across multiple spatial database dialects.

\paragraph{Template-Based Generation Framework}
The core generation engine operates through structured template definitions that encapsulate spatial query patterns:
\begin{enumerate}
    \item \textbf{Template Definition}: Each template specifies SQL structure, complexity level, geometry applicability, and natural language description
    \item \textbf{Parameter Substitution}: Systematic replacement of placeholders with contextually appropriate values
    \item \textbf{Dialect Adaptation}: Automatic translation between PostGIS and SpatiaLite syntax and function naming
    \item \textbf{Validation}: Geometric compatibility checking and syntactic verification
\end{enumerate}

\paragraph{Complexity Stratification}
Templates are organized across three complexity levels reflecting increasing spatial analysis sophistication:
\begin{itemize}
    \item \textbf{Level A (Basic)}: Fundamental spatial operations (point-in-polygon, distance filtering, basic measurements)
    \item \textbf{Level B (Intermediate)}: Multi-step spatial analysis (spatial joins with aggregation, reprojection workflows, geometric validation)
    \item \textbf{Level C (Advanced)}: Complex spatial algorithms (clustering analysis, network connectivity, cross-schema integration)
\end{itemize}

\paragraph{Cross-Dialect Compatibility}
Our generation framework ensures seamless compatibility between PostGIS and SpatiaLite through systematic function mapping and syntax adaptation:
\begin{itemize}
    \item \textbf{Function Name Mapping}: Automatic translation (e.g., \texttt{ST\_Length} $\rightarrow$ \texttt{GLength})
    \item \textbf{Syntax Adaptation}: Handling of dialect-specific constructs (geography types, KNN operators)
    \item \textbf{Feature Compatibility}: Graceful handling of dialect-specific functions (raster operations, 3D analysis)
\end{itemize}

\subsubsection{Schema-Specific Generation: CIM Wizard Integration}
\label{subsubsec:schema-specific}

To address the gap between synthetic training data and real-world database complexity, our pipeline incorporates schema-specific generators that produce contextually relevant training examples based on actual database schemas.

\paragraph{CIM Wizard Database Schema}
The CIM Wizard integration demonstrates schema-specific generation using a comprehensive Italian smart city infrastructure database with three primary schemas:
\begin{itemize}
    \item \textbf{cim\_vector}: Spatial infrastructure (buildings, electrical grid, project boundaries)
    \item \textbf{cim\_census}: Demographic and socioeconomic data (population statistics, housing characteristics)
    \item \textbf{cim\_raster}: Elevation and terrain data (DSM, DTM rasters for height calculation)
\end{itemize}

\paragraph{Cross-Schema Integration Templates}
The CIM Wizard generator includes 28 specialized templates enabling complex cross-schema analysis:
\begin{itemize}
    \item \textbf{Building-Infrastructure Analysis}: Proximity analysis between buildings and electrical grid infrastructure
    \item \textbf{Census-Building Correlation}: Spatial overlay analysis correlating building properties with demographic data
    \item \textbf{Raster-Vector Integration}: Building height calculation using DSM/DTM rasters clipped by building geometries
    \item \textbf{Multi-Schema Clustering}: Comprehensive spatial clustering across vector, census, and building datasets
\end{itemize}

\paragraph{Realistic Parameter Generation}
Schema-specific generators employ domain-aware parameter generation ensuring realistic query scenarios:
\begin{lstlisting}[language=python,caption={Domain-aware parameter generation example}]
# Building type distribution based on Italian census data
BUILDING_TYPES = {
    "residential": 0.7,    # 70% residential buildings
    "commercial": 0.15,    # 15% commercial
    "industrial": 0.10,    # 10% industrial
    "mixed_use": 0.05      # 5% mixed use
}

# Voltage level requirements for electrical grid analysis
VOLTAGE_REQUIREMENTS = {
    "low_voltage": "< 1 kV",
    "medium_voltage": "1-35 kV", 
    "high_voltage": "> 35 kV"
}
\end{lstlisting}

\subsubsection{Extensibility for Additional Database Schemas}
\label{subsubsec:extensibility}

Our architecture is designed for systematic extension to additional database schemas, enabling domain-specific training dataset generation across diverse spatial applications.

\paragraph{Modular Architecture}
The pipeline employs a modular design facilitating schema integration:
\begin{enumerate}
    \item \textbf{Schema Definition}: Declarative specification of table structures, geometry columns, and domain constraints
    \item \textbf{Template Specification}: Schema-specific templates referencing actual table and column names
    \item \textbf{Parameter Generation}: Domain-aware value generation reflecting realistic data distributions
    \item \textbf{Evidence Tracking}: Automatic capture of schema usage for training analysis
\end{enumerate}

\paragraph{Integration Framework}
New database schemas can be integrated through standardized interfaces:
\begin{itemize}
    \item \textbf{Schema Configuration}: JSON-based schema definition with metadata
    \item \textbf{Template Library}: Reusable spatial query patterns adaptable to different domains
    \item \textbf{Validation Framework}: Automated checking of schema compatibility and template validity
    \item \textbf{Evidence Generation}: Systematic tracking of database, schema, table, and function usage
\end{itemize}

\paragraph{Future Extensions}
The modular architecture supports integration of additional domain-specific schemas:
\begin{itemize}
    \item \textbf{Transportation Networks}: Road networks, public transit systems, logistics analysis
    \item \textbf{Environmental Monitoring}: Sensor networks, pollution analysis, climate data
    \item \textbf{Urban Planning}: Land use classification, zoning analysis, development planning
    \item \textbf{Healthcare Systems}: Hospital catchment areas, emergency response optimization
\end{itemize}

This extensible approach ensures our spatial SQL generation framework can adapt to diverse application domains while maintaining empirically-validated function selection and consistent training data quality.


\subsection{Evaluation Metrics} execution accuracy EX and execution match EM

\subsection{Implementation Details} base models:llama, qwen, deepseek
hyper parameters: r->16 alpha->256
target layers ->q-proj k-proj y-proj

\subsection{Environment} fine tunning: single NVIDIA RTX A5000 GPU (24 GB memory)
inference: Ollama


\subsection{baseline} different version of fine tunned LLM has been compared to eachother and openai gpt and 
This Stage~1 study is deliberately restricted to a schema-aware setting. Its primary aim is to demonstrate feasibility: that a fine-tuned open-source model can bridge natural language input and spatial SQL for a real-world database schema. In Stage~2, future work will expand the dataset to multiple spatial databases, enabling the creation of a small benchmark and the comparison of schema-aware and schema-agnostic approaches.